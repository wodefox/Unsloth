# Unsloth 模型微调工具使用手册

## 1. 软件介绍

Unsloth模型微调工具是一个图形化界面应用程序，专门用于帮助用户进行大语言模型的微调训练。无论您是人工智能初学者还是专业研究人员，都能通过这个工具轻松实现模型的个性化训练。

### 1.1 主要功能

- 支持多种主流大语言模型（Llama、Qwen、Mistral、Phi等）
- 提供简单直观的图形界面操作
- 支持模型的本地扫描和加载
- 可视化训练过程监控
- 灵活的参数配置选项

## 2. 界面布局

软件界面主要分为三个区域：

### 2.1 左侧设置面板

包含三个主要选项卡：

1. **基本设置**
   - 模型选择：选择模型系列和大小
   - 数据设置：配置训练数据和保存路径
   - 训练参数：设置基本训练参数
   - 训练状态：显示进度条和训练日志

2. **高级设置**
   - LoRA配置
   - 优化器选项
   - 学习率调度
   - 数据处理选项

3. **网络设置**
   - 离线模式配置
   - 代理设置
   - 连接参数调整

### 2.2 右侧可视化面板

展示实时训练过程的可视化图表：
- 训练损失曲线
- 学习率变化曲线

## 3. 基础使用流程

### 3.1 选择模型

1. 在"基本设置"选项卡中，首先选择模型系列（如Llama、Qwen等）
2. 选择对应的模型大小（如1B、3B、7B等）
3. 模型名称会自动更新，也可以手动修改

### 3.2 配置训练数据

1. 点击"选择文件"按钮，选择训练数据文件（JSON格式）
2. 设置模型保存路径，默认为"./output"

### 3.3 设置训练参数

基本参数包括：
- 学习率：默认2e-5
- Batch Size：默认4
- 训练轮数：默认3
- 最大长度：默认512

### 3.4 开始训练

1. 确认所有参数设置无误后，点击"开始训练"按钮
2. 观察训练进度条和日志输出
3. 可以通过右侧图表实时监控训练过程

## 4. 高级功能说明

### 4.1 LoRA训练

1. 在"高级设置"中启用LoRA选项
2. 设置LoRA rank值（默认8）
3. 可以显著减少显存占用，适合消费级显卡

### 4.2 优化器配置

- 支持多种优化器：adamw_8bit、adamw_torch、adam、sgd
- 可调整权重衰减值
- 支持不同的学习率调度策略

### 4.3 离线模式

1. 在"网络设置"中启用离线模式
2. 设置本地模型目录
3. 使用"扫描模型"功能加载本地模型

## 5. 常见问题解答

### 5.1 显存不足

解决方案：
- 减小batch size
- 启用LoRA训练
- 使用FP16混合精度训练
- 减小最大序列长度

### 5.2 训练速度慢

优化建议：
- 增加数据处理线程数
- 启用序列打包选项
- 适当增加batch size
- 使用梯度累积

### 5.3 模型加载失败

检查事项：
- 确认网络连接正常
- 验证模型路径正确
- 检查是否有足够磁盘空间
- 确认显存容量充足

## 6. 使用技巧

1. **参数调优建议**
   - 从小的学习率开始尝试
   - 根据显卡性能调整batch size
   - 监控训练损失避免过拟合

2. **数据准备建议**
   - 确保训练数据格式正确
   - 数据量建议在1000条以上
   - 注意数据质量和多样性

3. **资源优化**
   - 合理使用梯度累积
   - 适时清理缓存文件
   - 定期保存训练检查点

## 7. 快捷键

- 开始训练：无快捷键，点击按钮
- 暂停训练：无快捷键，点击按钮
- 停止训练：无快捷键，点击按钮
- 切换主题：在工具栏选择

## 8. 注意事项

1. 训练前注意事项：
   - 确保有足够的磁盘空间
   - 检查显存是否充足
   - 备份重要数据

2. 训练中注意事项：
   - 定期查看训练日志
   - 监控显存使用情况
   - 注意保存训练结果

3. 训练后注意事项：
   - 及时导出模型
   - 测试模型效果
   - 备份训练配置

## 9. 更新和维护

- 定期检查软件更新
- 保持训练数据的更新和优化
- 关注社区反馈和建议

## 10. 联系与支持

如果在使用过程中遇到问题，可以：
1. 查看训练日志获取错误信息
2. 检查显卡驱动是否最新
3. 确认Python环境配置正确
4. 参考Unsloth官方文档

祝您使用愉快！